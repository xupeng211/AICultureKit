#!/usr/bin/env python3
"""
æ–‡æ¡£è´¨é‡åˆ†æå·¥å…·
"""

import ast
import re
from pathlib import Path
from typing import Dict, List, Any
from dataclasses import dataclass
from collections import defaultdict


@dataclass
class DocumentationIssue:
    """æ–‡æ¡£é—®é¢˜"""

    file_path: str
    line_number: int
    issue_type: str
    severity: str
    description: str
    suggestion: str


class DocumentationAnalyzer:
    """æ–‡æ¡£åˆ†æå™¨"""

    def __init__(self, project_path: Path):
        """__init__å‡½æ•°"""
        self.project_path = project_path
        self.issues = []

    def analyze_documentation(self) -> Dict[str, Any]:
        """åˆ†ææ–‡æ¡£è´¨é‡"""
        print("ğŸ“š å¼€å§‹æ–‡æ¡£ä¸æ³¨é‡Šè´¨é‡åˆ†æ...")

        # åˆ†æPythonæ–‡ä»¶çš„æ–‡æ¡£å­—ç¬¦ä¸²
        self._analyze_python_docstrings()

        # åˆ†æMarkdownæ–‡æ¡£
        self._analyze_markdown_docs()

        # åˆ†æä»£ç æ³¨é‡Šè´¨é‡
        self._analyze_code_comments()

        return self._generate_documentation_report()

    def _analyze_python_docstrings(self):
        """åˆ†æPythonæ–‡æ¡£å­—ç¬¦ä¸²"""
        python_files = list(self.project_path.rglob("*.py"))

        for file_path in python_files:
            if self._should_skip_file(file_path):
                continue

            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    tree = ast.parse(content)

                self._check_module_docstring(file_path, tree)
                self._check_class_docstrings(file_path, tree)
                self._check_function_docstrings(file_path, tree)

            except (SyntaxError, UnicodeDecodeError):
                continue

    def _check_module_docstring(self, file_path: Path, tree: ast.AST):
        """æ£€æŸ¥æ¨¡å—æ–‡æ¡£å­—ç¬¦ä¸²"""
        module_docstring = ast.get_docstring(tree)

        if not module_docstring:
            self.issues.append(
                DocumentationIssue(
                    file_path=str(file_path),
                    line_number=1,
                    issue_type="missing_module_docstring",
                    severity="medium",
                    description="æ¨¡å—ç¼ºå°‘æ–‡æ¡£å­—ç¬¦ä¸²",
                    suggestion="æ·»åŠ æ¨¡å—çº§åˆ«çš„æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œè¯´æ˜æ¨¡å—çš„ç”¨é€”å’ŒåŠŸèƒ½",
                )
            )
        elif len(module_docstring.strip()) < 10:
            self.issues.append(
                DocumentationIssue(
                    file_path=str(file_path),
                    line_number=1,
                    issue_type="short_module_docstring",
                    severity="low",
                    description="æ¨¡å—æ–‡æ¡£å­—ç¬¦ä¸²è¿‡çŸ­",
                    suggestion="æ‰©å±•æ¨¡å—æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæä¾›æ›´è¯¦ç»†çš„è¯´æ˜",
                )
            )

    def _check_class_docstrings(self, file_path: Path, tree: ast.AST):
        """æ£€æŸ¥ç±»æ–‡æ¡£å­—ç¬¦ä¸²"""
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                class_docstring = ast.get_docstring(node)

                if not class_docstring:
                    self.issues.append(
                        DocumentationIssue(
                            file_path=str(file_path),
                            line_number=node.lineno,
                            issue_type="missing_class_docstring",
                            severity="medium",
                            description=f"ç±» {node.name} ç¼ºå°‘æ–‡æ¡£å­—ç¬¦ä¸²",
                            suggestion="æ·»åŠ ç±»æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œè¯´æ˜ç±»çš„ç”¨é€”ã€å±æ€§å’Œæ–¹æ³•",
                        )
                    )
                elif len(class_docstring.strip()) < 20:
                    self.issues.append(
                        DocumentationIssue(
                            file_path=str(file_path),
                            line_number=node.lineno,
                            issue_type="short_class_docstring",
                            severity="low",
                            description=f"ç±» {node.name} æ–‡æ¡£å­—ç¬¦ä¸²è¿‡çŸ­",
                            suggestion="æ‰©å±•ç±»æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæä¾›æ›´è¯¦ç»†çš„è¯´æ˜",
                        )
                    )

    def _check_function_docstrings(self, file_path: Path, tree: ast.AST):
        """æ£€æŸ¥å‡½æ•°æ–‡æ¡£å­—ç¬¦ä¸²"""
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                # è·³è¿‡ç§æœ‰æ–¹æ³•å’Œç‰¹æ®Šæ–¹æ³•
                if node.name.startswith("_"):
                    continue

                func_docstring = ast.get_docstring(node)

                if not func_docstring:
                    self.issues.append(
                        DocumentationIssue(
                            file_path=str(file_path),
                            line_number=node.lineno,
                            issue_type="missing_function_docstring",
                            severity="low",
                            description=f"å‡½æ•° {node.name} ç¼ºå°‘æ–‡æ¡£å­—ç¬¦ä¸²",
                            suggestion="æ·»åŠ å‡½æ•°æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œè¯´æ˜å‚æ•°ã€è¿”å›å€¼å’ŒåŠŸèƒ½",
                        )
                    )
                elif len(func_docstring.strip()) < 15:
                    self.issues.append(
                        DocumentationIssue(
                            file_path=str(file_path),
                            line_number=node.lineno,
                            issue_type="short_function_docstring",
                            severity="low",
                            description=f"å‡½æ•° {node.name} æ–‡æ¡£å­—ç¬¦ä¸²è¿‡çŸ­",
                            suggestion="æ‰©å±•å‡½æ•°æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæä¾›æ›´è¯¦ç»†çš„è¯´æ˜",
                        )
                    )

                # æ£€æŸ¥å‚æ•°æ–‡æ¡£
                if len(node.args.args) > 2:  # æœ‰å¤šä¸ªå‚æ•°çš„å‡½æ•°
                    if not self._has_parameter_docs(func_docstring):
                        self.issues.append(
                            DocumentationIssue(
                                file_path=str(file_path),
                                line_number=node.lineno,
                                issue_type="missing_parameter_docs",
                                severity="low",
                                description=f"å‡½æ•° {node.name} ç¼ºå°‘å‚æ•°æ–‡æ¡£",
                                suggestion="åœ¨æ–‡æ¡£å­—ç¬¦ä¸²ä¸­æ·»åŠ å‚æ•°è¯´æ˜",
                            )
                        )

    def _analyze_markdown_docs(self):
        """åˆ†æMarkdownæ–‡æ¡£"""
        md_files = list(self.project_path.rglob("*.md"))

        for file_path in md_files:
            if self._should_skip_file(file_path):
                continue

            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()

                self._check_markdown_structure(file_path, content)
                self._check_markdown_links(file_path, content)

            except UnicodeDecodeError:
                continue

    def _check_markdown_structure(self, file_path: Path, content: str):
        """æ£€æŸ¥Markdownç»“æ„"""
        lines = content.split("\n")

        # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡é¢˜
        has_title = any(line.startswith("#") for line in lines[:10])
        if not has_title:
            self.issues.append(
                DocumentationIssue(
                    file_path=str(file_path),
                    line_number=1,
                    issue_type="missing_title",
                    severity="medium",
                    description="Markdownæ–‡æ¡£ç¼ºå°‘æ ‡é¢˜",
                    suggestion="åœ¨æ–‡æ¡£å¼€å¤´æ·»åŠ ä¸»æ ‡é¢˜",
                )
            )

        # æ£€æŸ¥æ–‡æ¡£é•¿åº¦
        if len(content.strip()) < 100:
            self.issues.append(
                DocumentationIssue(
                    file_path=str(file_path),
                    line_number=1,
                    issue_type="short_document",
                    severity="low",
                    description="æ–‡æ¡£å†…å®¹è¿‡çŸ­",
                    suggestion="æ‰©å±•æ–‡æ¡£å†…å®¹ï¼Œæä¾›æ›´è¯¦ç»†çš„ä¿¡æ¯",
                )
            )

    def _check_markdown_links(self, file_path: Path, content: str):
        """æ£€æŸ¥Markdowné“¾æ¥"""
        # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        link_pattern = r"\[([^\]]+)\]\(([^)]+)\)"
        links = re.findall(link_pattern, content)

        for i, (text, url) in enumerate(links, 1):
            # æ£€æŸ¥ç›¸å¯¹è·¯å¾„é“¾æ¥æ˜¯å¦å­˜åœ¨
            if not url.startswith(("http://", "https://", "mailto:")):
                link_path = file_path.parent / url
                if not link_path.exists():
                    self.issues.append(
                        DocumentationIssue(
                            file_path=str(file_path),
                            line_number=i,
                            issue_type="broken_link",
                            severity="medium",
                            description=f"é“¾æ¥æŒ‡å‘ä¸å­˜åœ¨çš„æ–‡ä»¶: {url}",
                            suggestion="æ£€æŸ¥é“¾æ¥è·¯å¾„æ˜¯å¦æ­£ç¡®æˆ–åˆ›å»ºå¯¹åº”çš„æ–‡ä»¶",
                        )
                    )

    def _analyze_code_comments(self):
        """åˆ†æä»£ç æ³¨é‡Šè´¨é‡"""
        python_files = list(self.project_path.rglob("*.py"))

        for file_path in python_files:
            if self._should_skip_file(file_path):
                continue

            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    lines = f.readlines()

                self._check_comment_quality(file_path, lines)

            except UnicodeDecodeError:
                continue

    def _check_comment_quality(self, file_path: Path, lines: List[str]):
        """æ£€æŸ¥æ³¨é‡Šè´¨é‡"""
        for i, line in enumerate(lines, 1):
            stripped = line.strip()

            # æ£€æŸ¥TODOæ³¨é‡Š
            if re.search(r"#\s*TODO", stripped, re.IGNORECASE):
                if len(stripped) < 20:  # TODOæ³¨é‡Šè¿‡çŸ­
                    self.issues.append(
                        DocumentationIssue(
                            file_path=str(file_path),
                            line_number=i,
                            issue_type="short_todo_comment",
                            severity="low",
                            description="TODOæ³¨é‡Šè¿‡çŸ­ï¼Œç¼ºå°‘è¯¦ç»†è¯´æ˜",
                            suggestion="æ‰©å±•TODOæ³¨é‡Šï¼Œè¯´æ˜å…·ä½“éœ€è¦åšä»€ä¹ˆ",
                        )
                    )

            # æ£€æŸ¥å•è¡Œæ³¨é‡Š
            if stripped.startswith("#") and not stripped.startswith("##"):
                comment_text = stripped[1:].strip()
                if len(comment_text) < 5:
                    self.issues.append(
                        DocumentationIssue(
                            file_path=str(file_path),
                            line_number=i,
                            issue_type="short_comment",
                            severity="low",
                            description="æ³¨é‡Šè¿‡çŸ­ï¼Œç¼ºå°‘æœ‰ç”¨ä¿¡æ¯",
                            suggestion="æä¾›æ›´è¯¦ç»†çš„æ³¨é‡Šè¯´æ˜",
                        )
                    )

    def _has_parameter_docs(self, docstring: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å‚æ•°æ–‡æ¡£"""
        if not docstring:
            return False

        # ç®€å•æ£€æŸ¥æ˜¯å¦åŒ…å«å‚æ•°ç›¸å…³çš„å…³é”®è¯
        param_keywords = [
            "param",
            "parameter",
            "arg",
            "argument",
            "Args:",
            "Parameters:",
        ]
        return any(keyword in docstring for keyword in param_keywords)

    def _should_skip_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦è·³è¿‡æ–‡ä»¶"""
        skip_dirs = {"venv", "__pycache__", ".git", "node_modules", ".pytest_cache"}
        return any(part in skip_dirs for part in file_path.parts)

    def _generate_documentation_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ–‡æ¡£åˆ†ææŠ¥å‘Š"""
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
        by_severity = defaultdict(list)
        for issue in self.issues:
            by_severity[issue.severity].append(issue)

        # æŒ‰ç±»å‹åˆ†ç»„
        by_type = defaultdict(list)
        for issue in self.issues:
            by_type[issue.issue_type].append(issue)

        # æŒ‰æ–‡ä»¶åˆ†ç»„
        by_file = defaultdict(list)
        for issue in self.issues:
            by_file[issue.file_path].append(issue)

        return {
            "total_issues": len(self.issues),
            "by_severity": dict(by_severity),
            "by_type": dict(by_type),
            "by_file": dict(by_file),
            "summary": {
                "high_severity": len(by_severity["high"]),
                "medium_severity": len(by_severity["medium"]),
                "low_severity": len(by_severity["low"]),
                "files_with_issues": len(by_file),
            },
        }


def main():
    """mainå‡½æ•°"""
    analyzer = DocumentationAnalyzer(Path("."))
    report = analyzer.analyze_documentation()

    print("\nğŸ“š æ–‡æ¡£ä¸æ³¨é‡Šè´¨é‡åˆ†ææŠ¥å‘Š")
    print("=" * 50)

    summary = report["summary"]
    print(f"æ–‡æ¡£é—®é¢˜æ€»æ•°: {report['total_issues']}")
    print(f"é«˜ä¸¥é‡æ€§: {summary['high_severity']} ä¸ª")
    print(f"ä¸­ç­‰ä¸¥é‡æ€§: {summary['medium_severity']} ä¸ª")
    print(f"ä½ä¸¥é‡æ€§: {summary['low_severity']} ä¸ª")
    print(f"æœ‰é—®é¢˜çš„æ–‡ä»¶: {summary['files_with_issues']} ä¸ª")

    print("\nğŸ” æŒ‰é—®é¢˜ç±»å‹åˆ†ç»„:")
    for issue_type, issues in report["by_type"].items():
        print(f"  {issue_type}: {len(issues)} ä¸ª")

    print("\nğŸš¨ ä¸­ç­‰ä¸¥é‡æ€§é—®é¢˜è¯¦æƒ…:")
    medium_issues = report["by_severity"].get("medium", [])
    for i, issue in enumerate(medium_issues[:5], 1):
        print(f"  {i}. {issue.file_path}:{issue.line_number}")
        print(f"     {issue.description}")
        print(f"     å»ºè®®: {issue.suggestion}")
        print()

    if len(medium_issues) > 5:
        print(f"  ... è¿˜æœ‰ {len(medium_issues) - 5} ä¸ªä¸­ç­‰ä¸¥é‡æ€§é—®é¢˜")

    return report


if __name__ == "__main__":
    main()
