"""
æ€§èƒ½æ–‡åŒ–æ¨¡å— - æ€§èƒ½åŸºå‡†æµ‹è¯•ã€å›å½’æ£€æµ‹ã€å†…å­˜ç›‘æ§

æä¾›å…¨é¢çš„æ€§èƒ½æ–‡åŒ–ä¿éšœï¼ŒåŒ…æ‹¬ï¼š
1. æ€§èƒ½åŸºå‡†æµ‹è¯•æ¡†æ¶
2. æ€§èƒ½å›å½’æ£€æµ‹
3. å†…å­˜æ³„æ¼æ£€æµ‹
4. å“åº”æ—¶é—´ç›‘æ§
5. æ€§èƒ½ä¼˜åŒ–å»ºè®®
"""

import json
import threading
import time
import tracemalloc
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional

import psutil


@dataclass
class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æ•°æ®"""

    name: str
    category: str  # api, function, database, file_io
    baseline_time: float  # åŸºå‡†æ‰§è¡Œæ—¶é—´(ç§’)
    baseline_memory: int  # åŸºå‡†å†…å­˜ä½¿ç”¨(å­—èŠ‚)
    threshold_multiplier: float = 2.0  # é˜ˆå€¼å€æ•°
    created_at: float = field(default_factory=time.time)
    last_updated: float = field(default_factory=time.time)


@dataclass
class PerformanceResult:
    """æ€§èƒ½æµ‹è¯•ç»“æœ"""

    benchmark_name: str
    execution_time: float
    memory_usage: int
    cpu_usage: float
    is_regression: bool
    regression_factor: float
    timestamp: float = field(default_factory=time.time)
    details: Dict[str, Any] = field(default_factory=dict)


class PerformanceProfiler:
    """æ€§èƒ½åˆ†æå™¨"""

    def __init__(self):
        """__init__å‡½æ•°"""
        self.active_profiles = {}
        self.results = []

    def start_profiling(self, name: str) -> None:
        """å¼€å§‹æ€§èƒ½åˆ†æ"""
        tracemalloc.start()
        self.active_profiles[name] = {
            'start_time': time.perf_counter(),
            'start_memory': tracemalloc.get_traced_memory()[0],
            'process': psutil.Process(),
            'cpu_start': psutil.cpu_percent(),
        }

    def stop_profiling(self, name: str) -> PerformanceResult:
        """åœæ­¢æ€§èƒ½åˆ†æå¹¶è¿”å›ç»“æœ"""
        if name not in self.active_profiles:
            raise ValueError(f"No active profile found for {name}")

        profile = self.active_profiles[name]
        end_time = time.perf_counter()
        current_memory, peak_memory = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        execution_time = end_time - profile['start_time']
        memory_usage = peak_memory - profile['start_memory']
        cpu_usage = psutil.cpu_percent() - profile['cpu_start']

        result = PerformanceResult(
            benchmark_name=name,
            execution_time=execution_time,
            memory_usage=memory_usage,
            cpu_usage=cpu_usage,
            is_regression=False,  # å°†ç”±åŸºå‡†æ¯”è¾ƒç¡®å®š
            regression_factor=1.0,
            details={
                'peak_memory': peak_memory,
                'current_memory': current_memory,
                'process_memory': profile['process'].memory_info().rss,
            },
        )

        del self.active_profiles[name]
        self.results.append(result)
        return result


class PerformanceBenchmarkManager:
    """æ€§èƒ½åŸºå‡†ç®¡ç†å™¨"""

    def __init__(self, project_path: Path):
        """__init__å‡½æ•°"""
        self.project_path = project_path
        self.benchmarks_file = (
            project_path / ".aiculture" / "performance_benchmarks.json"
        )
        self.results_file = project_path / ".aiculture" / "performance_results.json"
        self.benchmarks: Dict[str, PerformanceBenchmark] = {}
        self.profiler = PerformanceProfiler()
        self._load_benchmarks()

    def _load_benchmarks(self) -> None:
        """åŠ è½½æ€§èƒ½åŸºå‡†æ•°æ®"""
        if self.benchmarks_file.exists():
            try:
                with open(self.benchmarks_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for name, benchmark_data in data.items():
                        self.benchmarks[name] = PerformanceBenchmark(**benchmark_data)
            except Exception as e:
                print(f"åŠ è½½æ€§èƒ½åŸºå‡†æ•°æ®å¤±è´¥: {e}")

    def _save_benchmarks(self) -> None:
        """ä¿å­˜æ€§èƒ½åŸºå‡†æ•°æ®"""
        self.benchmarks_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.benchmarks_file, 'w', encoding='utf-8') as f:
            data = {
                name: {
                    'name': b.name,
                    'category': b.category,
                    'baseline_time': b.baseline_time,
                    'baseline_memory': b.baseline_memory,
                    'threshold_multiplier': b.threshold_multiplier,
                    'created_at': b.created_at,
                    'last_updated': b.last_updated,
                }
                for name, b in self.benchmarks.items()
            }
            json.dump(data, f, indent=2, ensure_ascii=False)

    def create_benchmark(
        self, name: str, category: str, func: Callable, *args, **kwargs
    ) -> PerformanceBenchmark:
        """åˆ›å»ºæ€§èƒ½åŸºå‡†"""
        print(f"ğŸƒ åˆ›å»ºæ€§èƒ½åŸºå‡†: {name}")

        # è¿è¡Œå¤šæ¬¡å–å¹³å‡å€¼
        times = []
        memories = []

        for i in range(3):
            self.profiler.start_profiling(f"{name}_baseline_{i}")
            func(*args, **kwargs)
            result = self.profiler.stop_profiling(f"{name}_baseline_{i}")
            times.append(result.execution_time)
            memories.append(result.memory_usage)

        baseline_time = sum(times) / len(times)
        baseline_memory = sum(memories) / len(memories)

        benchmark = PerformanceBenchmark(
            name=name,
            category=category,
            baseline_time=baseline_time,
            baseline_memory=baseline_memory,
        )

        self.benchmarks[name] = benchmark
        self._save_benchmarks()

        print(f"âœ… åŸºå‡†åˆ›å»ºå®Œæˆ: {baseline_time:.4f}s, {baseline_memory}bytes")
        return benchmark

    def run_benchmark(
        self, name: str, func: Callable, *args, **kwargs
    ) -> PerformanceResult:
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        if name not in self.benchmarks:
            raise ValueError(f"Benchmark {name} not found")

        benchmark = self.benchmarks[name]

        self.profiler.start_profiling(name)
        func(*args, **kwargs)
        result = self.profiler.stop_profiling(name)

        # æ£€æŸ¥æ˜¯å¦æœ‰æ€§èƒ½å›å½’
        time_regression = result.execution_time > (
            benchmark.baseline_time * benchmark.threshold_multiplier
        )
        memory_regression = result.memory_usage > (
            benchmark.baseline_memory * benchmark.threshold_multiplier
        )

        result.is_regression = time_regression or memory_regression
        result.regression_factor = max(
            result.execution_time / benchmark.baseline_time,
            (
                result.memory_usage / benchmark.baseline_memory
                if benchmark.baseline_memory > 0
                else 1.0
            ),
        )

        self._save_result(result)
        return result

    def _save_result(self, result: PerformanceResult) -> None:
        """ä¿å­˜æ€§èƒ½æµ‹è¯•ç»“æœ"""
        self.results_file.parent.mkdir(parents=True, exist_ok=True)

        results = []
        if self.results_file.exists():
            try:
                with open(self.results_file, 'r', encoding='utf-8') as f:
                    results = json.load(f)
            except Exception:
                results = []

        results.append(
            {
                'benchmark_name': result.benchmark_name,
                'execution_time': result.execution_time,
                'memory_usage': result.memory_usage,
                'cpu_usage': result.cpu_usage,
                'is_regression': result.is_regression,
                'regression_factor': result.regression_factor,
                'timestamp': result.timestamp,
                'details': result.details,
            }
        )

        # åªä¿ç•™æœ€è¿‘100ä¸ªç»“æœ
        results = results[-100:]

        with open(self.results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        if not self.results_file.exists():
            return {'benchmarks': 0, 'regressions': 0, 'results': []}

        with open(self.results_file, 'r', encoding='utf-8') as f:
            results = json.load(f)

        regressions = [r for r in results if r.get('is_regression', False)]

        return {
            'total_benchmarks': len(self.benchmarks),
            'total_results': len(results),
            'regressions': len(regressions),
            'regression_rate': len(regressions) / len(results) if results else 0,
            'recent_results': results[-10:],
            'worst_regressions': sorted(
                regressions, key=lambda x: x.get('regression_factor', 1), reverse=True
            )[:5],
        }


class MemoryLeakDetector:
    """å†…å­˜æ³„æ¼æ£€æµ‹å™¨"""

    def __init__(self):
        """__init__å‡½æ•°"""
        self.snapshots = []
        self.monitoring = False
        self.monitor_thread = None

    def start_monitoring(self, interval: int = 60) -> None:
        """å¼€å§‹å†…å­˜ç›‘æ§"""
        if self.monitoring:
            return

        self.monitoring = True
        self.monitor_thread = threading.Thread(
            target=self._monitor_memory, args=(interval,), daemon=True
        )
        self.monitor_thread.start()
        print(f"ğŸ” å¼€å§‹å†…å­˜ç›‘æ§ï¼Œé—´éš”: {interval}ç§’")

    def stop_monitoring(self) -> None:
        """åœæ­¢å†…å­˜ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        print("â¹ï¸ å†…å­˜ç›‘æ§å·²åœæ­¢")

    def _monitor_memory(self, interval: int) -> None:
        """å†…å­˜ç›‘æ§çº¿ç¨‹"""
        while self.monitoring:
            try:
                process = psutil.Process()
                memory_info = process.memory_info()

                snapshot = {
                    'timestamp': time.time(),
                    'rss': memory_info.rss,  # ç‰©ç†å†…å­˜
                    'vms': memory_info.vms,  # è™šæ‹Ÿå†…å­˜
                    'percent': process.memory_percent(),
                    'open_files': len(process.open_files()),
                    'connections': len(process.connections()),
                }

                self.snapshots.append(snapshot)

                # åªä¿ç•™æœ€è¿‘1000ä¸ªå¿«ç…§
                if len(self.snapshots) > 1000:
                    self.snapshots = self.snapshots[-1000:]

                time.sleep(interval)

            except Exception as e:
                print(f"å†…å­˜ç›‘æ§é”™è¯¯: {e}")
                time.sleep(interval)

    def detect_leaks(self) -> Dict[str, Any]:
        """æ£€æµ‹å†…å­˜æ³„æ¼"""
        if len(self.snapshots) < 10:
            return {
                'status': 'insufficient_data',
                'message': 'æ•°æ®ä¸è¶³ï¼Œæ— æ³•æ£€æµ‹å†…å­˜æ³„æ¼',
            }

        # åˆ†æå†…å­˜è¶‹åŠ¿
        recent_snapshots = self.snapshots[-50:]  # æœ€è¿‘50ä¸ªå¿«ç…§

        rss_values = [s['rss'] for s in recent_snapshots]
        vms_values = [s['vms'] for s in recent_snapshots]

        # è®¡ç®—å†…å­˜å¢é•¿è¶‹åŠ¿
        rss_trend = self._calculate_trend(rss_values)
        vms_trend = self._calculate_trend(vms_values)

        # æ£€æµ‹å¼‚å¸¸å¢é•¿
        leak_detected = False
        warnings = []

        if rss_trend > 1024 * 1024:  # 1MB/åˆ†é’Ÿå¢é•¿
            leak_detected = True
            warnings.append(f"ç‰©ç†å†…å­˜æŒç»­å¢é•¿: {rss_trend / 1024 / 1024:.2f}MB/åˆ†é’Ÿ")

        if vms_trend > 10 * 1024 * 1024:  # 10MB/åˆ†é’Ÿå¢é•¿
            leak_detected = True
            warnings.append(f"è™šæ‹Ÿå†…å­˜æŒç»­å¢é•¿: {vms_trend / 1024 / 1024:.2f}MB/åˆ†é’Ÿ")

        return {
            'status': 'leak_detected' if leak_detected else 'normal',
            'rss_trend': rss_trend,
            'vms_trend': vms_trend,
            'warnings': warnings,
            'current_memory': recent_snapshots[-1] if recent_snapshots else None,
            'snapshots_analyzed': len(recent_snapshots),
        }

    def _calculate_trend(self, values: List[float]) -> float:
        """è®¡ç®—è¶‹åŠ¿ï¼ˆç®€å•çº¿æ€§å›å½’æ–œç‡ï¼‰"""
        if len(values) < 2:
            return 0

        n = len(values)
        x_sum = sum(range(n))
        y_sum = sum(values)
        xy_sum = sum(i * values[i] for i in range(n))
        x2_sum = sum(i * i for i in range(n))

        # è®¡ç®—æ–œç‡
        slope = (n * xy_sum - x_sum * y_sum) / (n * x2_sum - x_sum * x_sum)
        return slope


def performance_benchmark(name: str, category: str = "function"):
    """æ€§èƒ½åŸºå‡†è£…é¥°å™¨"""

    def decorator(func: Callable):
        """decoratorå‡½æ•°"""

        def wrapper(*args, **kwargs):
            """wrapperå‡½æ•°"""
            # è¿™é‡Œå¯ä»¥é›†æˆåˆ°å…¨å±€çš„æ€§èƒ½ç®¡ç†å™¨ä¸­
            return func(*args, **kwargs)

        wrapper._performance_benchmark = {'name': name, 'category': category}
        return wrapper

    return decorator


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    manager = PerformanceBenchmarkManager(Path("."))

    @performance_benchmark("test_function", "function")
    def test_function():
        """test_functionå‡½æ•°"""
        time.sleep(0.1)
        return sum(range(1000))

    # åˆ›å»ºåŸºå‡†
    manager.create_benchmark("test_function", "function", test_function)

    # è¿è¡Œæµ‹è¯•
    result = manager.run_benchmark("test_function", test_function)
    print(f"æ€§èƒ½æµ‹è¯•ç»“æœ: {result}")

    # è·å–æŠ¥å‘Š
    report = manager.get_performance_report()
    print(f"æ€§èƒ½æŠ¥å‘Š: {report}")
