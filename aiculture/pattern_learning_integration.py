"""
æ¨¡å¼å­¦ä¹ é›†æˆç³»ç»Ÿ - ç»“åˆAIå­¦ä¹ ç³»ç»Ÿå’Œå¤šè¯­è¨€åˆ†æï¼Œæä¾›ç»¼åˆçš„é¡¹ç›®æ¨¡å¼å­¦ä¹ ã€‚

è¿™ä¸ªæ¨¡å—å°†:
1. æ•´åˆPythonå’Œå…¶ä»–è¯­è¨€çš„åˆ†æç»“æœ
2. è¯†åˆ«è·¨è¯­è¨€çš„ä¸€è‡´æ€§æ¨¡å¼
3. ç”Ÿæˆé’ˆå¯¹å¤šè¯­è¨€é¡¹ç›®çš„ä¸ªæ€§åŒ–è§„åˆ™
4. æä¾›è¯­è¨€é—´çš„æœ€ä½³å®è·µå»ºè®®
"""

import json
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any

from .ai_learning_system import AILearningEngine, LearningResult
from .multi_language_analyzer import LanguageMetrics, MultiLanguageManager


@dataclass
class CrossLanguagePattern:
    """è·¨è¯­è¨€æ¨¡å¼"""

    pattern_name: str
    languages: list[str]
    pattern_values: dict[str, Any]
    consistency_score: float
    recommendation: str


@dataclass
class IntegratedLearningResult:
    """é›†æˆå­¦ä¹ ç»“æœ"""

    python_learning: LearningResult | None
    multi_language_analysis: dict[str, LanguageMetrics]
    cross_language_patterns: list[CrossLanguagePattern]
    unified_recommendations: list[str]
    overall_maturity: str
    recommended_strictness: float
    language_specific_rules: dict[str, dict[str, Any]]
    generated_at: float


class PatternLearningIntegrator:
    """æ¨¡å¼å­¦ä¹ é›†æˆå™¨"""

    def __init__(self, project_path: Path) -> None:
        """åˆå§‹åŒ–æ¨¡å¼å­¦ä¹ é›†æˆå™¨"""
        self.project_path = project_path
        self.ai_learning_engine = AILearningEngine(project_path)
        self.multi_language_manager = MultiLanguageManager(project_path)

    def perform_comprehensive_learning(self) -> IntegratedLearningResult:
        """æ‰§è¡Œç»¼åˆå­¦ä¹ åˆ†æ"""
        # 1. æ‰§è¡ŒPython AIå­¦ä¹ 
        try:
            python_learning = self.ai_learning_engine.learn_project_patterns()
        except Exception:
            python_learning = None

        # 2. æ‰§è¡Œå¤šè¯­è¨€åˆ†æ
        multi_language_analysis = self.multi_language_manager.analyze_all_languages()

        # 3. åˆ†æè·¨è¯­è¨€æ¨¡å¼
        cross_language_patterns = self._analyze_cross_language_patterns(
            python_learning, multi_language_analysis
        )

        # 4. è¯„ä¼°æ•´ä½“é¡¹ç›®æˆç†Ÿåº¦
        overall_maturity = self._assess_overall_maturity(python_learning, multi_language_analysis)

        # 5. è®¡ç®—æ¨èä¸¥æ ¼åº¦
        recommended_strictness = self._calculate_unified_strictness(
            python_learning, multi_language_analysis, overall_maturity
        )

        # 6. ç”Ÿæˆè¯­è¨€ç‰¹å®šè§„åˆ™
        language_specific_rules = self._generate_language_specific_rules(
            multi_language_analysis, cross_language_patterns
        )

        # 7. ç”Ÿæˆç»Ÿä¸€å»ºè®®
        unified_recommendations = self._generate_unified_recommendations(
            python_learning, multi_language_analysis, cross_language_patterns
        )

        return IntegratedLearningResult(
            python_learning=python_learning,
            multi_language_analysis=multi_language_analysis,
            cross_language_patterns=cross_language_patterns,
            unified_recommendations=unified_recommendations,
            overall_maturity=overall_maturity,
            recommended_strictness=recommended_strictness,
            language_specific_rules=language_specific_rules,
            generated_at=time.time(),
        )

    def _analyze_cross_language_patterns(
        self,
        python_learning: LearningResult | None,
        multi_lang_analysis: dict[str, LanguageMetrics],
    ) -> list[CrossLanguagePattern]:
        """åˆ†æè·¨è¯­è¨€æ¨¡å¼"""
        patterns = []

        # æ”¶é›†æ‰€æœ‰è¯­è¨€çš„æ¨¡å¼
        all_patterns = {}

        # æ·»åŠ Pythonæ¨¡å¼
        if python_learning:
            for pattern in python_learning.patterns:
                pattern_key = pattern.pattern_name
                if pattern_key not in all_patterns:
                    all_patterns[pattern_key] = {}
                all_patterns[pattern_key]["python"] = pattern.pattern_value

        # æ·»åŠ å…¶ä»–è¯­è¨€æ¨¡å¼
        for lang_name, metrics in multi_lang_analysis.items():
            for pattern in metrics.patterns:
                pattern_key = pattern.pattern_name
                if pattern_key not in all_patterns:
                    all_patterns[pattern_key] = {}
                all_patterns[pattern_key][lang_name] = pattern.pattern_value

        # åˆ†ææ¨¡å¼ä¸€è‡´æ€§
        for pattern_name, language_values in all_patterns.items():
            if len(language_values) > 1:  # å¤šç§è¯­è¨€éƒ½æœ‰è¿™ä¸ªæ¨¡å¼
                consistency_score = self._calculate_pattern_consistency(language_values)

                # ç”Ÿæˆå»ºè®®
                recommendation = self._generate_pattern_recommendation(
                    pattern_name, language_values, consistency_score
                )

                patterns.append(
                    CrossLanguagePattern(
                        pattern_name=pattern_name,
                        languages=list(language_values.keys()),
                        pattern_values=language_values,
                        consistency_score=consistency_score,
                        recommendation=recommendation,
                    )
                )

        # åˆ†æç‰¹æ®Šçš„è·¨è¯­è¨€æ¨¡å¼
        patterns.extend(self._analyze_special_cross_patterns(python_learning, multi_lang_analysis))

        return patterns

    def _calculate_pattern_consistency(self, language_values: dict[str, Any]) -> float:
        """è®¡ç®—æ¨¡å¼ä¸€è‡´æ€§åˆ†æ•°"""
        if len(language_values) <= 1:
            return 1.0

        # å¯¹äºå‘½åé£æ ¼ç­‰åˆ†ç±»å€¼
        if all(isinstance(v, str) for v in language_values.values()):
            unique_values = set(language_values.values())
            return 1.0 if len(unique_values) == 1 else 0.5

        # å¯¹äºæ•°å€¼ç±»å‹
        if all(isinstance(v, (int, float)) for v in language_values.values()):
            values = list(language_values.values())
            if not values:
                return 0.0

            mean_val = sum(values) / len(values)
            if mean_val == 0:
                return 1.0 if all(v == 0 for v in values) else 0.0

            # è®¡ç®—ç›¸å¯¹æ ‡å‡†å·®
            variance = sum((v - mean_val) ** 2 for v in values) / len(values)
            std_dev = variance**0.5
            relative_std = std_dev / abs(mean_val) if mean_val != 0 else 0

            # è½¬æ¢ä¸ºä¸€è‡´æ€§åˆ†æ•° (0-1)
            consistency = max(0, 1 - relative_std)
            return consistency

        # å¯¹äºå¸ƒå°”å€¼
        if all(isinstance(v, bool) for v in language_values.values()):
            unique_values = set(language_values.values())
            return 1.0 if len(unique_values) == 1 else 0.0

        # é»˜è®¤æƒ…å†µ
        return 0.5

    def _generate_pattern_recommendation(
        self,
        pattern_name: str,
        language_values: dict[str, Any],
        consistency_score: float,
    ) -> str:
        """ç”Ÿæˆæ¨¡å¼å»ºè®®"""
        if consistency_score >= 0.8:
            return f"âœ… {pattern_name}åœ¨æ‰€æœ‰è¯­è¨€ä¸­ä¿æŒè‰¯å¥½ä¸€è‡´æ€§ï¼Œç»§ç»­ç»´æŒå½“å‰æ ‡å‡†"
        elif consistency_score >= 0.5:
            return f"âš ï¸ {pattern_name}åœ¨ä¸åŒè¯­è¨€é—´å­˜åœ¨è½»å¾®å·®å¼‚ï¼Œå»ºè®®ç»Ÿä¸€æ ‡å‡†"
        else:
            return f"âŒ {pattern_name}åœ¨ä¸åŒè¯­è¨€é—´å·®å¼‚è¾ƒå¤§ï¼Œéœ€è¦åˆ¶å®šç»Ÿä¸€çš„è·¨è¯­è¨€è§„èŒƒ"

    def _analyze_special_cross_patterns(
        self,
        python_learning: LearningResult | None,
        multi_lang_analysis: dict[str, LanguageMetrics],
    ) -> list[CrossLanguagePattern]:
        """åˆ†æç‰¹æ®Šçš„è·¨è¯­è¨€æ¨¡å¼"""
        patterns = []

        # å¤æ‚åº¦å¯¹æ¯”æ¨¡å¼
        complexity_values = {}
        if python_learning:
            # ä»Pythonå­¦ä¹ ç»“æœä¸­è·å–å¤æ‚åº¦ä¿¡æ¯
            for pattern in python_learning.patterns:
                if pattern.pattern_type == "complexity":
                    complexity_values["python"] = pattern.pattern_value

        for lang_name, metrics in multi_lang_analysis.items():
            complexity_values[lang_name] = metrics.avg_complexity

        if len(complexity_values) > 1:
            consistency = self._calculate_pattern_consistency(complexity_values)
            patterns.append(
                CrossLanguagePattern(
                    pattern_name="complexity_consistency",
                    languages=list(complexity_values.keys()),
                    pattern_values=complexity_values,
                    consistency_score=consistency,
                    recommendation=self._generate_complexity_recommendation(
                        complexity_values, consistency
                    ),
                )
            )

        # å‡½æ•°å¤§å°å¯¹æ¯”æ¨¡å¼
        function_size_values = {}
        for lang_name, metrics in multi_lang_analysis.items():
            function_size_values[lang_name] = metrics.avg_function_size

        if function_size_values:
            consistency = self._calculate_pattern_consistency(function_size_values)
            patterns.append(
                CrossLanguagePattern(
                    pattern_name="function_size_consistency",
                    languages=list(function_size_values.keys()),
                    pattern_values=function_size_values,
                    consistency_score=consistency,
                    recommendation=self._generate_function_size_recommendation(
                        function_size_values, consistency
                    ),
                )
            )

        return patterns

    def _generate_complexity_recommendation(
        self, complexity_values: dict[str, float], consistency: float
    ) -> str:
        """ç”Ÿæˆå¤æ‚åº¦å»ºè®®"""
        avg_complexity = sum(complexity_values.values()) / len(complexity_values)

        if consistency >= 0.8:
            if avg_complexity <= 5:
                return "âœ… æ‰€æœ‰è¯­è¨€çš„å¤æ‚åº¦éƒ½ä¿æŒåœ¨ç†æƒ³èŒƒå›´å†…ä¸”ä¸€è‡´"
            else:
                return "âš ï¸ æ‰€æœ‰è¯­è¨€çš„å¤æ‚åº¦éƒ½åé«˜ï¼Œå»ºè®®æ•´ä½“ç®€åŒ–é€»è¾‘"
        else:
            max_lang = max(complexity_values.items(), key=lambda x: x[1])
            min_lang = min(complexity_values.items(), key=lambda x: x[1])
            return f"âŒ å¤æ‚åº¦ä¸ä¸€è‡´ï¼š{max_lang[0]}({max_lang[1]:.1f}) vs {min_lang[0]}({min_lang[1]:.1f})ï¼Œéœ€è¦å¹³è¡¡å„è¯­è¨€çš„å®ç°å¤æ‚åº¦"

    def _generate_function_size_recommendation(
        self, size_values: dict[str, float], consistency: float
    ) -> str:
        """ç”Ÿæˆå‡½æ•°å¤§å°å»ºè®®"""
        avg_size = sum(size_values.values()) / len(size_values)

        if consistency >= 0.8:
            if avg_size <= 20:
                return "âœ… æ‰€æœ‰è¯­è¨€çš„å‡½æ•°å¤§å°éƒ½ä¿æŒåœ¨åˆç†èŒƒå›´å†…ä¸”ä¸€è‡´"
            else:
                return "âš ï¸ æ‰€æœ‰è¯­è¨€çš„å‡½æ•°éƒ½åå¤§ï¼Œå»ºè®®æ‹†åˆ†é•¿å‡½æ•°"
        else:
            max_lang = max(size_values.items(), key=lambda x: x[1])
            min_lang = min(size_values.items(), key=lambda x: x[1])
            return f"âŒ å‡½æ•°å¤§å°ä¸ä¸€è‡´ï¼š{max_lang[0]}({max_lang[1]:.1f}è¡Œ) vs {min_lang[0]}({min_lang[1]:.1f}è¡Œ)ï¼Œå»ºè®®ç»Ÿä¸€å‡½æ•°é•¿åº¦æ ‡å‡†"

    def _assess_overall_maturity(
        self,
        python_learning: LearningResult | None,
        multi_lang_analysis: dict[str, LanguageMetrics],
    ) -> str:
        """è¯„ä¼°æ•´ä½“é¡¹ç›®æˆç†Ÿåº¦"""
        maturity_scores = []

        # Pythonæˆç†Ÿåº¦
        if python_learning:
            python_score = self._maturity_to_score(python_learning.project_maturity)
            maturity_scores.append(python_score)

        # å…¶ä»–è¯­è¨€æˆç†Ÿåº¦è¯„ä¼°
        for lang_name, metrics in multi_lang_analysis.items():
            lang_score = self._calculate_language_maturity_score(metrics)
            maturity_scores.append(lang_score)

        if not maturity_scores:
            return "beginner"

        avg_score = sum(maturity_scores) / len(maturity_scores)

        if avg_score >= 0.8:
            return "expert"
        elif avg_score >= 0.6:
            return "intermediate"
        else:
            return "beginner"

    def _maturity_to_score(self, maturity: str) -> float:
        """å°†æˆç†Ÿåº¦ç­‰çº§è½¬æ¢ä¸ºåˆ†æ•°"""
        mapping = {"beginner": 0.3, "intermediate": 0.6, "expert": 0.9}
        return mapping.get(maturity, 0.3)

    def _calculate_language_maturity_score(self, metrics: LanguageMetrics) -> float:
        """è®¡ç®—è¯­è¨€ç‰¹å®šçš„æˆç†Ÿåº¦åˆ†æ•°"""
        score = 0.0
        weight_sum = 0.0

        # å‘½åä¸€è‡´æ€§ (æƒé‡: 0.3)
        score += metrics.naming_consistency * 0.3
        weight_sum += 0.3

        # é£æ ¼ä¸€è‡´æ€§ (æƒé‡: 0.2)
        score += metrics.style_consistency * 0.2
        weight_sum += 0.2

        # å¤æ‚åº¦è¯„åˆ† (æƒé‡: 0.3)
        complexity_score = (
            1.0 if metrics.avg_complexity <= 5 else max(0, 1 - (metrics.avg_complexity - 5) / 10)
        )
        score += complexity_score * 0.3
        weight_sum += 0.3

        # å‡½æ•°å¤§å°è¯„åˆ† (æƒé‡: 0.2)
        size_score = (
            1.0
            if metrics.avg_function_size <= 20
            else max(0, 1 - (metrics.avg_function_size - 20) / 30)
        )
        score += size_score * 0.2
        weight_sum += 0.2

        return score / weight_sum if weight_sum > 0 else 0.0

    def _calculate_unified_strictness(
        self,
        python_learning: LearningResult | None,
        multi_lang_analysis: dict[str, LanguageMetrics],
        overall_maturity: str,
    ) -> float:
        """è®¡ç®—ç»Ÿä¸€çš„ä¸¥æ ¼åº¦"""
        base_strictness = {"beginner": 0.6, "intermediate": 0.75, "expert": 0.9}

        strictness = base_strictness.get(overall_maturity, 0.7)

        # æ ¹æ®è·¨è¯­è¨€ä¸€è‡´æ€§è°ƒæ•´
        if len(multi_lang_analysis) > 1:
            consistency_scores = []

            # è®¡ç®—å‘½åä¸€è‡´æ€§
            naming_styles = []
            for metrics in multi_lang_analysis.values():
                naming_patterns = [p for p in metrics.patterns if "naming" in p.pattern_name]
                if naming_patterns:
                    naming_styles.extend([p.pattern_value for p in naming_patterns])

            if naming_styles:
                unique_styles = set(naming_styles)
                naming_consistency = 1.0 if len(unique_styles) == 1 else 0.5
                consistency_scores.append(naming_consistency)

            # å¦‚æœæœ‰ä¸€è‡´æ€§æ•°æ®ï¼Œè°ƒæ•´ä¸¥æ ¼åº¦
            if consistency_scores:
                avg_consistency = sum(consistency_scores) / len(consistency_scores)
                if avg_consistency >= 0.8:
                    strictness += 0.05  # é«˜ä¸€è‡´æ€§ï¼Œå¯ä»¥æé«˜ä¸¥æ ¼åº¦
                elif avg_consistency < 0.5:
                    strictness -= 0.1  # ä½ä¸€è‡´æ€§ï¼Œé™ä½ä¸¥æ ¼åº¦é¿å…è¿‡åº¦é™åˆ¶

        return max(0.3, min(1.0, strictness))

    def _generate_language_specific_rules(
        self,
        multi_lang_analysis: dict[str, LanguageMetrics],
        cross_patterns: list[CrossLanguagePattern],
    ) -> dict[str, dict[str, Any]]:
        """ç”Ÿæˆè¯­è¨€ç‰¹å®šçš„è§„åˆ™"""
        language_rules = {}

        for lang_name, metrics in multi_lang_analysis.items():
            rules = {}

            # åŸºäºè¯­è¨€ç‰¹å®šæ¨¡å¼ç”Ÿæˆè§„åˆ™
            for pattern in metrics.patterns:
                if pattern.confidence > 0.7:
                    if pattern.pattern_type == "naming":
                        rules[f"enforce_{pattern.pattern_name}"] = {
                            "enabled": True,
                            "style": pattern.pattern_value,
                            "severity": "warning",
                        }
                    elif pattern.pattern_type == "style":
                        rules[f"style_{pattern.pattern_name}"] = {
                            "enabled": True,
                            "preference": pattern.pattern_value,
                            "severity": "info",
                        }

            # åŸºäºå¤æ‚åº¦ç”Ÿæˆè§„åˆ™
            if metrics.avg_complexity > 0:
                rules["complexity_threshold"] = {
                    "enabled": True,
                    "max_complexity": max(5, int(metrics.avg_complexity * 1.2)),
                    "severity": "warning",
                }

            # åŸºäºå‡½æ•°å¤§å°ç”Ÿæˆè§„åˆ™
            if metrics.avg_function_size > 0:
                rules["function_length_threshold"] = {
                    "enabled": True,
                    "max_lines": max(20, int(metrics.avg_function_size * 1.5)),
                    "severity": "info",
                }

            language_rules[lang_name] = rules

        return language_rules

    def _generate_python_recommendations(self, python_learning: LearningResult) -> list[str]:
        """ç”ŸæˆPythonç›¸å…³å»ºè®®"""
        recommendations = []

        if python_learning.confidence > 0.8:
            recommendations.append("Pythonä»£ç è´¨é‡å¾ˆé«˜ï¼Œå»ºè®®ä¿æŒå½“å‰æ ‡å‡†")
        elif python_learning.confidence > 0.6:
            recommendations.append("Pythonä»£ç è´¨é‡è‰¯å¥½ï¼Œå¯è¿›ä¸€æ­¥ä¼˜åŒ–")
        else:
            recommendations.append("Pythonä»£ç éœ€è¦é‡ç‚¹æ”¹è¿›")

        # åŸºäºå­¦ä¹ åˆ°çš„æ¨¡å¼æ·»åŠ å…·ä½“å»ºè®®
        for pattern in python_learning.patterns[:3]:  # åªå–å‰3ä¸ªæœ€é‡è¦çš„æ¨¡å¼
            if pattern.confidence > 0.7:
                recommendations.append(f"å»ºè®®åœ¨é¡¹ç›®ä¸­æ¨å¹¿ {pattern.pattern_type} æ¨¡å¼")

        return recommendations

    def _generate_multilang_recommendations(
        self, multi_lang_analysis: dict[str, LanguageMetrics]
    ) -> list[str]:
        """ç”Ÿæˆå¤šè¯­è¨€ç›¸å…³å»ºè®®"""
        recommendations = []
        total_languages = len(multi_lang_analysis)

        if total_languages > 3:
            recommendations.append("é¡¹ç›®ä½¿ç”¨å¤šç§ç¼–ç¨‹è¯­è¨€ï¼Œå»ºè®®å»ºç«‹ç»Ÿä¸€çš„ä»£ç è§„èŒƒ")

        # åˆ†æå„è¯­è¨€çš„è´¨é‡
        high_quality_langs = []
        low_quality_langs = []

        for lang, metrics in multi_lang_analysis.items():
            if metrics.quality_score > 0.8:
                high_quality_langs.append(lang)
            elif metrics.quality_score < 0.5:
                low_quality_langs.append(lang)

        if high_quality_langs:
            recommendations.append(f"ä»¥ä¸‹è¯­è¨€ä»£ç è´¨é‡è¾ƒé«˜: {', '.join(high_quality_langs)}")

        if low_quality_langs:
            recommendations.append(f"ä»¥ä¸‹è¯­è¨€éœ€è¦é‡ç‚¹æ”¹è¿›: {', '.join(low_quality_langs)}")

        return recommendations

    def _generate_cross_pattern_recommendations(
        self, cross_patterns: list[CrossLanguagePattern]
    ) -> list[str]:
        """ç”Ÿæˆè·¨è¯­è¨€æ¨¡å¼å»ºè®®"""
        recommendations = []

        if not cross_patterns:
            recommendations.append("å»ºè®®å»ºç«‹è·¨è¯­è¨€çš„ç»Ÿä¸€å¼€å‘æ¨¡å¼")
            return recommendations

        # åˆ†æè·¨è¯­è¨€æ¨¡å¼
        strong_patterns = [p for p in cross_patterns if p.strength > 0.7]
        weak_patterns = [p for p in cross_patterns if p.strength < 0.4]

        if strong_patterns:
            recommendations.append(f"å‘ç° {len(strong_patterns)} ä¸ªå¼ºè·¨è¯­è¨€æ¨¡å¼ï¼Œå»ºè®®ç»§ç»­å¼ºåŒ–")

        if weak_patterns:
            recommendations.append(f"å‘ç° {len(weak_patterns)} ä¸ªå¼±è·¨è¯­è¨€æ¨¡å¼ï¼Œå»ºè®®æ”¹è¿›ä¸€è‡´æ€§")

        return recommendations

    def _generate_unified_recommendations(
        self,
        python_learning: LearningResult | None,
        multi_lang_analysis: dict[str, LanguageMetrics],
        cross_patterns: list[CrossLanguagePattern],
    ) -> list[str]:
        """ç”Ÿæˆç»Ÿä¸€å»ºè®®"""
        recommendations = []

        # åŸºäºPythonå­¦ä¹ ç»“æœçš„å»ºè®®
        if python_learning:
            recommendations.extend(self._generate_python_recommendations(python_learning))

        # åŸºäºå¤šè¯­è¨€åˆ†æçš„å»ºè®®
        recommendations.extend(self._generate_multilang_recommendations(multi_lang_analysis))

        # åŸºäºè·¨è¯­è¨€æ¨¡å¼çš„å»ºè®®
        recommendations.extend(self._generate_cross_pattern_recommendations(cross_patterns))

        # åŸºäºå¤æ‚åº¦çš„å»ºè®®
        complexity_pattern = next(
            (p for p in cross_patterns if p.pattern_name == "complexity_consistency"),
            None,
        )
        if complexity_pattern and complexity_pattern.consistency_score < 0.6:
            recommendations.append("ğŸ”„ ä¸åŒè¯­è¨€é—´çš„å¤æ‚åº¦å·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®å¹³è¡¡å®ç°å¤æ‚åº¦")

        # åŸºäºè¯­è¨€ç‰¹å®šç‰¹å¾çš„å»ºè®®
        for lang_name, metrics in multi_lang_analysis.items():
            if lang_name == "javascript":
                # JavaScriptç‰¹å®šå»ºè®®
                ts_patterns = [p for p in metrics.patterns if "typescript" in p.pattern_name]
                if ts_patterns and any(p.pattern_value < 0.5 for p in ts_patterns):
                    recommendations.append("ğŸ“ è€ƒè™‘å¢åŠ TypeScriptä½¿ç”¨æ¯”ä¾‹ï¼Œæå‡ç±»å‹å®‰å…¨æ€§")

        # åŸºäºæ•´ä½“è´¨é‡çš„å»ºè®®
        avg_naming_consistency = sum(
            m.naming_consistency for m in multi_lang_analysis.values()
        ) / len(multi_lang_analysis)
        if avg_naming_consistency < 0.8:
            recommendations.append("ğŸ“ å‘½åè§„èŒƒä¸å¤Ÿä¸€è‡´ï¼Œå»ºè®®ä¸ºæ¯ç§è¯­è¨€åˆ¶å®šæ˜ç¡®çš„å‘½åæ ‡å‡†")

        avg_style_consistency = sum(
            m.style_consistency for m in multi_lang_analysis.values()
        ) / len(multi_lang_analysis)
        if avg_style_consistency < 0.8:
            recommendations.append("ğŸ¨ ä»£ç é£æ ¼ä¸å¤Ÿç»Ÿä¸€ï¼Œå»ºè®®ä¸ºæ¯ç§è¯­è¨€é…ç½®ä»£ç æ ¼å¼åŒ–å·¥å…·")

        return recommendations


def save_integrated_learning_result(result: IntegratedLearningResult, project_path: Path) -> None:
    """ä¿å­˜é›†æˆå­¦ä¹ ç»“æœ"""
    result_file = project_path / ".aiculture" / "integrated_learning_result.json"
    result_file.parent.mkdir(parents=True, exist_ok=True)

    try:
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_result = asdict(result)

        with open(result_file, "w", encoding="utf-8") as f:
            json.dump(serializable_result, f, indent=2, default=str)
    except (OSError, TypeError):
        pass


def load_integrated_learning_result(
    project_path: Path,
) -> IntegratedLearningResult | None:
    """åŠ è½½é›†æˆå­¦ä¹ ç»“æœ"""
    result_file = project_path / ".aiculture" / "integrated_learning_result.json"

    try:
        if result_file.exists():
            with open(result_file, encoding="utf-8") as f:
                data = json.load(f)
                # è¿™é‡Œéœ€è¦å¤æ‚çš„ååºåˆ—åŒ–é€»è¾‘ï¼Œç®€åŒ–å¤„ç†
                return data  # è¿”å›å­—å…¸æ ¼å¼
    except (OSError, json.JSONDecodeError):
        pass

    return None
